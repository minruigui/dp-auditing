{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "padding_idx = 1\n",
    "bos_idx = 0\n",
    "eos_idx = 2\n",
    "max_seq_len = 256\n",
    "xlmr_vocab_path = r\"https://download.pytorch.org/models/text/xlmr.vocab.pt\"\n",
    "xlmr_spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(xlmr_spm_model_path),\n",
    "    T.VocabTransform(load_state_dict_from_url(xlmr_vocab_path)),\n",
    "    T.Truncate(max_seq_len - 2),\n",
    "    T.AddToken(token=bos_idx, begin=True),\n",
    "    T.AddToken(token=eos_idx, begin=False),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import SST2\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_datapipe = SST2(split=\"train\")\n",
    "dev_datapipe = SST2(split=\"dev\")\n",
    "\n",
    "\n",
    "# Transform the raw dataset using non-batched API (i.e apply transformation line by line)\n",
    "def apply_transform(x):\n",
    "    return text_transform(x[0]), x[1]\n",
    "\n",
    "\n",
    "train_datapipe = train_datapipe.map(apply_transform)\n",
    "train_datapipe = train_datapipe.batch(batch_size)\n",
    "train_datapipe = train_datapipe.rows2columnar([\"token_ids\", \"target\"])\n",
    "train_dataloader = DataLoader(train_datapipe, batch_size=None)\n",
    "\n",
    "dev_datapipe = dev_datapipe.map(apply_transform)\n",
    "dev_datapipe = dev_datapipe.batch(batch_size)\n",
    "dev_datapipe = dev_datapipe.rows2columnar([\"token_ids\", \"target\"])\n",
    "dev_dataloader = DataLoader(dev_datapipe, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return self.fc(hidden[-1].squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(250002, 100, padding_idx=1)\n",
       "  (rnn): RNN(100, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# Assuming the vocab object has a len() method\n",
    "DEVICE = torch.device('mps')\n",
    "vocab_size = len(load_state_dict_from_url(xlmr_vocab_path))\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "num_layers = 1\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim,\n",
    "                 output_dim, padding_idx, num_layers=num_layers)\n",
    "\n",
    "# Loss and optimizer\n",
    "# Combines a sigmoid layer and the BCE loss in one single class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_IterDataPipeSerializationWrapper instance doesn't have valid length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/datapipe.py:363\u001b[0m, in \u001b[0;36m_DataPipeSerializationWrapper.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datapipe)\n\u001b[1;32m    364\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torchdata/datapipes/iter/util/rows2columnar.py:78\u001b[0m, in \u001b[0;36mRows2ColumnarIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_datapipe)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/grouping.py:84\u001b[0m, in \u001b[0;36mBatcherIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatapipe) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/callable.py:127\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe, Sized):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatapipe)\n\u001b[1;32m    128\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/sharding.py:81\u001b[0m, in \u001b[0;36mShardingFilterIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_datapipe, Sized):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_datapipe) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_of_instances \u001b[39m+\u001b[39m\\\n\u001b[1;32m     82\u001b[0m         (\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance_id \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_datapipe) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_of_instances) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combinatorics.py:139\u001b[0m, in \u001b[0;36mShufflerIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe, Sized):\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatapipe)\n\u001b[1;32m    140\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/callable.py:128\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe)\n\u001b[0;32m--> 128\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: MapperIterDataPipe instance doesn't have valid length",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/project/auditing-dpsgd/embedding.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/project/auditing-dpsgd/embedding.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/project/auditing-dpsgd/embedding.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/project/auditing-dpsgd/embedding.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:474\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m    458\u001b[0m         \u001b[39m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m         \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m         \u001b[39m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m         length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset)  \u001b[39m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[1;32m    475\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[1;32m    476\u001b[0m             \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m ceil\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/datapipes/datapipe.py:365\u001b[0m, in \u001b[0;36m_DataPipeSerializationWrapper.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datapipe)\n\u001b[1;32m    364\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m instance doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have valid length\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _IterDataPipeSerializationWrapper instance doesn't have valid length"
     ]
    }
   ],
   "source": [
    "import torchtext.functional as F\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input = F.to_tensor(\n",
    "                        batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n",
    "        target = torch.tensor(batch[\"target\"]).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(train_dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
